% !TEX TS-program = xelatex
\documentclass[12pt]{article}

\input{.house-style/preamble.tex}
\input{local-preamble.tex}

\title{The ``Expert Tool'' Trap:\\ Notes on an AI-assisted Methods Paper}
\author{Brett Reynolds \orcidlink{0000-0003-0073-7195}\\
  Humber Polytechnic \& University of Toronto\\
  \href{mailto:brett.reynolds@humber.ca}{brett.reynolds@humber.ca}}
\date{\today}

\begin{document}
\maketitle

\section{Introduction}

This document reflects on the process of writing \emph{Grammatical Variation Meta-Study} with an AI agent. The initial goal was efficiency: delegate the ``boring'' parts of Bayesian modeling and LaTeX formatting to a machine. The reality was less about acceleration and more about collision. The agent did not simply implement a pre-existing plan; it exposed the fragility of that plan through its own failures.

This log details three specific crises---technical, epistemic, and rhetorical---that forced a deviation from the standard research workflow. It suggests that the value of AI in research is not as a \mention{force multiplier} but as an \mention{adversarial partner}.

\section{The ``Expert Tool'' Trap (Technical)}

The most dangerous moment in the project occurred on January 10, when I attempted to fit a joint selection--outcome model using the industry-standard package \texttt{brms}. 

The logic was sound: I needed to model the probability of a variable being \term{tested} ($y_1$) and, conditional on that, the probability of it being \term{found} ($y_2$). A multivariate model seemed the obvious choice. The agent wrote the code, the model compiled, the chains converged ($R < 1.01$), and estimates were precise.

They were also wrong.

As noted in the pedagogical log (2026-01-10 \mention{Multivariate brms Missingness Check}), the \texttt{brms} package, essentially an expert system for regression, assumes by default that missing values in outcomes should be dropped (listwise deletion). Because $y_2$ is undefined for untested variables, the package silently dropped all rows where $y_1 = 0$. The resulting model was mathematically valid but scientifically vacuous: it modeled selection only among those already selected.

The agent, trained to produce ``correct'' code, produced code that was syntactically perfect but semantically fatal. The fix was not better prompting. It was a retreat to first principles: writing a custom Stan model to explicitly define the likelihood for the untested case. The agent could write the Stan code, but only after the human identified that the ``easy'' path was a dead end.

\section{Simulation as Critique (Epistemic)}

The second crisis was conceptual. The early drafts of the paper framed the high success rate of grammatical variables as evidence of ``human sociolinguistic capacity.'' It was a celebration of the field's ability to find meaning everywhere.

To test this framing, I ran a \mention{Simulated Reviewer} session (2026-01-10 \mention{Simulated Reviewer Critiques}), generating feedback from personas representing distinct epistemic communities (e.g., \mention{Zimmer} for variationist theory, \mention{Gelman} for statistics).

The critique was blistering. The \mention{Godfrey-Smith} persona pointed out a fundamental confusion between the map and the territory:
\begin{quote}
``You are confusing the capacity of the organism with the filter of the institution. A high success rate in published papers doesn't mean language is saturated with social meaning; it means researchers are rational actors who don't publish nulls.''
\end{quote}

This was not a copy-edit. It was a refutation. It forced a rewrite of the Abstract and Discussion (2026-01-10 \mention{Drafting Learnings Reflection}), shifting the claim from a linguistic discovery to a sociological one. The AI, usually a compliance machine, became a mechanism for estrangement, allowing me to see the paper's flaws through an outsider's eyes before they reached a human reviewer.

\section{Rhetoric as Debugging (Synthesis)}

The final lesson was that code and prose are not separate domains. In trying to explain the selection model to a general audience, I found myself relying on lazy academic connectives: \mention{Thus}, \mention{Therefore}, \mention{It is worth noting}.

The agent's enforcement of ``plain style'' (2026-01-10 \mention{Plain Style Enforcement}) revealed that these connectives were often masking logical gaps. If a sentence couldn't stand without a \mention{Therefore}, the logical connection usually wasn't in the data.

I replaced these with classical rhetorical figures. For example, to sharpen the distinction between the two stages of the model, I used \term{symploce} (repeating the start and end of clauses):
\begin{itemize}
    \item \emph{Selection asks}: Among all possible variables, which are chosen for study?
    \item \emph{Outcome asks}: Among those chosen, which are reported as significant?
\end{itemize}
This was not decoration. It was debugging. The constraint of the rhetorical form forced a precision in the conceptual structure that the code had missed.

\section{Conclusion: The ``Sudo'' Boundary}

The project concluded with a minor operational check: the agent paused before running a \texttt{sudo} command (2026-01-10 \mention{Privileged Actions Check-In}). This boundary remains the best metaphor for the collaboration. The agent can suggest, code, simulated, and critique. But it cannot start the machine, and it cannot sign the paper.

The efficiency narrative promises that AI will do the work. The reality of this project was that AI created \emph{more} work---more code to check, more critiques to answer, more drafts to refine. But it was better work. The paper is more rigorous, more humble, and more true than the one I would have written alone.

\end{document}
