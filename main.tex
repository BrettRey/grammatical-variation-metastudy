% !TEX TS-program = xelatex
\documentclass[12pt]{article}

\input{.house-style/preamble.tex}
\input{local-preamble.tex}

\title{Grammatical Variation Meta-Study:\\ A Bayesian Reanalysis}
\author{Brett Reynolds \orcidlink{0000-0003-0073-7195}\\
  Humber Polytechnic \& University of Toronto\\
  \href{mailto:brett.reynolds@humber.ca}{brett.reynolds@humber.ca}}
\date{}

\begin{document}
\maketitle

\begin{abstract}
\textcite{mackenzie2025spelling} catalog grammatical variables in sociolinguistic journals and find that realization variables dominate, while order variables are understudied and largely untested for social significance. Their descriptive tallies and coding scheme are valuable contributions. This paper offers a methodological refinement: a Bayesian selection--outcome model that explicitly estimates the probability of testing rather than treating untested variables as missing data. The key finding is that selection, not capacity, is the bottleneck. Testing rates differ sharply by journal and variation type, but conditional on testing, success rates are high and comparable (~88--94\%) across types. This pattern suggests that the asymmetry in the published record reflects research-design choices rather than a fundamental difference in social-meaning capacity between realization and order variables.
\end{abstract}

\section{Methods}

\subsection{Data and coding}
This reanalysis uses the open database compiled by \textcite{mackenzie2025spelling}, covering \emph{Language Variation and Change} (LVC) and the \emph{Journal of Sociolinguistics} (JSlx) from each journal's first year of publication through \liningnums{2023}. The unit of analysis is a variable--variety pairing. If a paper studies multiple variables or a variable across multiple varieties, each counts separately. The dataset contains \liningnums{427} variable--variety observations. Materials and code are available at \href{https://github.com/BrettRey/grammatical-variation-metastudy}{github.com/BrettRey/grammatical-variation-metastudy}.

Coding follows \textcite{mackenzie2025spelling}, which adapts the form--order--omission scheme of \textcite{mansfield2023dialect} and treats omission as a subtype of realization. Variables are included if they express grammatical meanings or functions in more than one way. Phonetic or phonological variables, lexical or discourse-pragmatic choice, discourse or conversation structure, and code-switching are excluded. The ambiguous variables (\mention{ING}) and (\mention{TD}) are also excluded. Variables are classified as \term{realization}, \term{order}, or \term{both}, with omission treated as a subtype of realization. The label \enquote{both} is used when a variable involves realization and order (for example, the dative alternation).

Social significance is coded as \enquote{not investigated}, \enquote{investigated but not found}, or \enquote{found}. When investigated, evidence is recorded as production, perception, or metalinguistic behaviors. In modeling, \enquote{tested} corresponds to \enquote{investigated}, and \enquote{found} is defined conditional on testing. Note that \enquote{found} means \enquote{reported as found}; the category \enquote{investigated but not found} is heterogeneous, encompassing genuinely absent effects, underpowered tests, and mismatched social-meaning targets. Because \enquote{both} is a coding convention rather than a theoretical category, a sensitivity check counts such variables in both realization and order categories; this doesn't change the qualitative selection pattern.

\subsection{Why a selection model?}
Variationists often ask whether grammatical variables carry social meaning. But the published record answers a different question: among variables that researchers chose to test, which showed effects? Variables that nobody tested are invisible to any analysis that conditions on the tested subset.

This creates a form of survivor bias. If order variables are rarely tested~-- perhaps because they're harder to elicit or less salient to researchers~-- then even a null result in the published record tells us little about their social-meaning potential. A \term{selection model} makes this explicit by estimating two things at once: the probability that a variable gets tested, and the probability that a tested variable shows an effect.

\subsection{Modeling strategy}
The model has two stages. The \term{selection stage} asks: given a variable--variety pairing in the corpus, was social significance tested? The \term{outcome stage} asks: given that testing occurred, was social significance found? This structure separates research-practice effects (who gets tested) from capacity effects (who shows effects once tested).

Readers familiar with mixed-effects logistic regression~-- the workhorse of variable-rule analysis in tools like Rbrul or Goldvarb~-- will recognize the outcome stage. It's the same model: a binary outcome (found or not) predicted by fixed effects (journal, variation type, year) and random effects (paper, author, language). What's new is pairing it with a selection stage that asks why some variables were tested at all.

Predictors in both stages are journal, variation type, and year. Year is z-scored (centered and scaled), so year odds ratios represent the change associated with a one-standard-deviation increase in publication year~-- roughly a decade in this corpus. Random intercepts for paper, first author, and language account for clustering, and correlations across stages allow the same paper or author to influence both testing and findings.

The model is fit in Stan, a probabilistic programming language for Bayesian inference. Priors are regularizing: Normal$(0, 1.0)$ for intercepts, Normal$(0, 0.5)$ for slopes, Exponential$(3)$ for random-effect standard deviations, and LKJ$(2)$ for correlation matrices. A sensitivity run centers intercept priors on observed marginal rates. Sampling uses four chains, \\liningnums{4000} iterations (\\liningnums{2000} warmup), and a brms two-stage baseline provides comparison.

\subsection{Reading the output}
Results are presented as odds ratios (ORs) with 95\% credible intervals (CrIs). An OR of 2 means twice the odds; an OR of 0.5 means half the odds. A 95\% CrI is the Bayesian analog of a confidence interval: it's the range within which the true value lies with 95\% probability, given the data and model. If the interval doesn't include 1, the effect is reliably different from no effect.

Posterior predictive checks (PPCs) ask: \enquote{Can the model reproduce data like ours?} If simulated data from the fitted model cluster around the observed data, the model is capturing the key structure. If not, something is missing.

One feature of Bayesian models with random effects is \term{partial pooling}: estimates for groups with little data are pulled toward the overall mean. This protects against overfitting small samples. Wide intervals for sparse categories (like order variables in perception studies) should be read as \enquote{data can't resolve this}, not \enquote{effect is absent.}

\section{Results}

\subsection{Model fit and predictive checks}
Posterior predictive checks (PPCs) ask whether replicated data from the fitted model resemble the observed data. Here, the replicated tested and found rates cluster around the observed rates overall and within journal and variation-type strata (Figures~\ref{fig:ppc-overall} and~\ref{fig:ppc-stratified}). The observed rates fall well inside the predictive distributions, suggesting that the model captures the main selection structure without apparent overfitting. Population-level predicted probabilities make the selection patterns easier to read (Figure~\ref{fig:probabilities}).

\begin{figure}[t]
  \centering
  \includegraphics[width=\textwidth]{ppc-overall.png}
  \caption{Posterior predictive checks for overall tested and found rates. Vertical lines mark the observed rates.}
  \label{fig:ppc-overall}
\end{figure}

\begin{figure}[t]
  \centering
  \includegraphics[width=\textwidth]{ppc-stratified.png}
  \caption{Stratified posterior predictive checks by journal and variation type. Vertical lines mark the observed rates.}
  \label{fig:ppc-stratified}
\end{figure}

\begin{figure}[t]
  \centering
  \includegraphics[width=\textwidth]{probabilities.png}
  \caption{Population-level predicted probabilities by journal (color) and variation type. Points show posterior medians with 95\% credible intervals.}
  \label{fig:probabilities}
\end{figure}

\subsection{Selection (tested)}
Odds ratios (ORs) and credible intervals (CrIs) summarize the selection stage. Values above 1 mean higher odds of being tested, and values below 1 mean lower odds. Testing varies by journal, variation type, and year. LVC is less likely to test variables than JSlx (OR $\approx 0.37$, 95\% CrI 0.17--0.83). Realization variables are more likely to be tested than order variables (OR $\approx 2.33$, 95\% CrI 1.12--4.89). Testing increases over time (OR $\approx 1.65$, 95\% CrI 1.04--2.67). The order--both contrast remains uncertain.

\subsection{Outcome (found \texorpdfstring{$\mid$}{|} tested)}
Conditional on testing, success rates are high and comparable across types. The probability of finding social significance is high (approximately 88--94\% across subgroups) and shows wide overlap across journals and variation types. Outcome effects have intervals that span no effect, consistent with selection rather than capacity as the bottleneck. Wide credible intervals in sparse categories~-- order variables, perception and metalinguistic domains~-- reflect data limitations, not absence of effect. The model's partial pooling shrinks extreme estimates toward the population mean, so intervals spanning no effect should be read as \enquote{data can't resolve this}, not \enquote{effect is absent.}

\subsection{Robustness}
The brms two-stage baseline yields the same qualitative pattern: selection effects are stable, outcome effects are weak. The centered-intercept sensitivity run reproduces the same conclusions.

An extended model adds a journal $\times$ variation-type interaction in the selection stage. If institutional framing requirements drive selection, JSlx might show a larger gap between realization and order than LVC. The interaction coefficients represent the multiplicative change in the LVC effect for each variation type relative to the reference (both). These coefficients have wide intervals spanning no effect (order:LVC OR $\approx 0.54$, 95\% CrI 0.23--1.29; realization:LVC OR $\approx 0.66$, 95\% CrI 0.30--1.47), so the data don't clearly support differential selection by journal.

A piecewise-linear year effect (two interior knots) tests whether selection trends are non-linear. The spline coefficients point in the same direction (ORs 1.19, 1.72, 1.32; all intervals spanning 1), consistent with the primary model's linear year assumption rather than distinct period effects.

\section{Discussion}

\subsection{Selection vs. outcome}
The main asymmetry is in selection. Journals, variable type, and time shape which variables are tested, but tested variables show high and overlapping probabilities of social significance. This supports a selection-driven account of the literature: the bottleneck is which variables are studied, not whether tested variables yield positive findings once investigated. This conclusion concerns publication and research-design practices rather than the underlying capacity of grammatical variables to carry social meaning.

\subsection{Implications for the Grammatical Invisibility Principle}
The Grammatical Invisibility Principle (GIP) holds that grammatical variables are less accessible to social evaluation than phonological variables. \textcite{mackenzie2025spelling} conclude that for realization variables, robust evidence of social significance exists; for order variables, absence of evidence doesn't equal evidence of absence. The present analysis sharpens this: \emph{conditional on testing}, both realization and order variables succeed at comparable rates (88--94\%). If order variables were genuinely invisible to social evaluation, one would expect lower success rates among those that are tested. Instead, the comparable rates suggest that the asymmetry in the published record reflects selection into testing, not a capacity difference. This finding refutes one version of the GIP~-- that order variables \emph{can't} carry social meaning~-- while remaining agnostic about whether order variables are \emph{harder to perceive} or \emph{less frequently recruited} for social work.

\subsection{Relation to prior descriptive work}
The descriptive goal of \textcite{mackenzie2025spelling} is to catalog grammatical variables and assess the state of evidence for social significance in each domain. The present analysis retains their coding scheme and tallies without modification. The points of agreement are substantial: realization variables dominate the literature; order variables are understudied; perception and metalinguistic domains remain sparse.

The focused revision is methodological. MacKenzie and Robinson devote considerable discussion (their Section 5) to selection into testing, including the file drawer problem and the fact that LVC doesn't require a social-significance framing. They correctly note that this selection makes any claim about order variables provisional. What the present analysis adds is an explicit model for this selection mechanism. Rather than treating untested variables as missing data to be ignored, the joint model estimates the probability of testing and conditions the outcome analysis on testing. This quantifies what MacKenzie and Robinson discuss narratively: order variables are much less likely to be tested, and the scarcity of perception/metalinguistic studies in order variables limits conclusions about outcome differences.

The novelty is thus formalization rather than conceptual revision. The selection--outcome decomposition provides a principled statistical framework for claims that MacKenzie and Robinson already make informally. The difference in emphasis is slight: the high baseline success rate (~88--94\%) is the story, and the between-type comparison is null in part because of ceiling effects and limited order data.

\subsection{Limitations and future work}
Order variables are rare and heavily concentrated in LVC, so journal comparisons are constrained. Outcome effects are data-limited in sparse categories, especially perception and metalinguistic domains; with only 31 tested order variables, power to detect plausible outcome-stage differences is limited. Because the database indexes published studies, the model can't address file-drawer effects where testing occurs but null results aren't published. Future work should expand the corpus beyond two journals and target perception and metalinguistic studies for order variables. It also makes sense to explore alternative temporal structures and to refine the classification of multi-locus variables.

\section*{Acknowledgements}

ChatGPT 5.2 assisted with initial data analysis, model development, and manuscript drafting. Claude Opus 4.5 assisted with revisions and house-style application.

\newpage
\appendix
\section{Model specification}

The joint selection--outcome model has two stages. For observation $i$:

\textbf{Selection stage:}
\begin{equation}
\text{tested}_i \sim \text{Bernoulli}(\text{logit}^{-1}(\eta^s_i))
\end{equation}
\begin{equation}
\eta^s_i = \alpha^s + \mathbf{x}_i \boldsymbol{\beta}^s + u^s_{\text{paper}[i]} + u^s_{\text{author}[i]} + u^s_{\text{language}[i]}
\end{equation}

\textbf{Outcome stage} (conditional on $\text{tested}_i = 1$):
\begin{equation}
\text{found}_i \mid \text{tested}_i = 1 \sim \text{Bernoulli}(\text{logit}^{-1}(\eta^o_i))
\end{equation}
\begin{equation}
\eta^o_i = \alpha^o + \mathbf{x}_i \boldsymbol{\beta}^o + u^o_{\text{paper}[i]} + u^o_{\text{author}[i]} + u^o_{\text{language}[i]}
\end{equation}

where $\mathbf{x}_i$ is the row of the fixed-effects design matrix containing journal, variation type, and year (z-scored).

\textbf{Random effects:} For each grouping factor $g \in \{\text{paper}, \text{author}, \text{language}\}$:
\begin{equation}
\begin{pmatrix} u^s_{g[i]} \\ u^o_{g[i]} \end{pmatrix} \sim \text{MVN}\left(\mathbf{0}, \boldsymbol{\Sigma}_g\right), \quad \boldsymbol{\Sigma}_g = \begin{pmatrix} \sigma^s_g & \rho_g \sigma^s_g \sigma^o_g \\ \rho_g \sigma^s_g \sigma^o_g & \sigma^o_g \end{pmatrix}
\end{equation}

\textbf{Priors:}
\begin{align}
\alpha^s, \alpha^o &\sim \text{Normal}(0, 1.0) \\
\beta^s_k, \beta^o_k &\sim \text{Normal}(0, 0.5) \\
\sigma^s_g, \sigma^o_g &\sim \text{Exponential}(3) \\
\mathbf{L}_g &\sim \text{LKJ-Cholesky}(2)
\end{align}

where $\mathbf{L}_g$ is the Cholesky factor of the correlation matrix for grouping factor $g$.

\newpage
\printbibliography

\end{document}
