% !TEX TS-program = xelatex
\documentclass[12pt]{article}

\input{.house-style/preamble.tex}
\input{local-preamble.tex}

\title{Grammatical Variation Meta-Study:\\ A Bayesian Reanalysis}
\author{Brett Reynolds \orcidlink{0000-0003-0073-7195}\\
  Humber Polytechnic \& University of Toronto\\
  \href{mailto:brett.reynolds@humber.ca}{brett.reynolds@humber.ca}}
\date{}

\begin{document}
\maketitle

\begin{abstract}
Sociolinguistic theory often assumes that some parts of grammar (like word order) are invisible to social judgment, while others (like pronunciation) carry social meaning. The empirical record seems to support this: realization variables dominate the literature, while order variables are rare. But is this asymmetry a fact about language, or a fact about research practice? This paper reanalyzes the \textcite{mackenzie2025spelling} database using a Bayesian selection--outcome model. The model explicitly distinguishes the probability of being tested from the probability of finding an effect. The key finding is that selection, not capacity, is the bottleneck in the published record. Testing rates differ sharply by journal and variation type, but conditional on testing, success rates are high and comparable (approximately 0.88--0.94 across subgroups). This suggests that the apparent scarcity of social meaning in order variables reflects a filter on what gets studied, not a fundamental incapacity of the variables themselves.
\end{abstract}

\section{Methods}

\subsection{Data and coding}
This reanalysis uses the open database compiled by \textcite{mackenzie2025spelling}, covering \emph{Language Variation and Change} (LVC) and the \emph{Journal of Sociolinguistics} (JSlx) from each journal's first year of publication through \liningnums{2023}. The unit of analysis is a variable--variety pairing. If a paper studies multiple variables or a variable across multiple varieties, each counts separately. The dataset contains \liningnums{427} variable--variety observations.

Materials and code are available at \href{https://github.com/BrettRey/grammatical-variation-metastudy}{github.com/BrettRey/grammatical-variation-metastudy}.

Coding follows \textcite{mackenzie2025spelling}, which adapts the form--order--omission scheme of \textcite{mansfield2023dialect} and treats omission as a subtype of realization. Variables are included if they express grammatical meanings or functions in more than one way. Phonetic or phonological variables, lexical or discourse-pragmatic choice, discourse or conversation structure, and code-switching are excluded. The ambiguous variables (\mention{ING}) and (\mention{TD}) are also excluded. Variables are classified as \term{realization}, \term{order}, or \term{both}, with omission treated as a subtype of realization. The label \enquote{both} is used when a variable involves realization and order (for example, the dative alternation).

Social significance is coded as \enquote{not investigated}, \enquote{investigated but not found}, or \enquote{found}.\footnote{The published CSV uses NA for both \enquote{not investigated} and \enquote{investigated but not found}. Following the original analysis code, NAs in the social-significance fields are recoded as \enquote{investigated but not found} when evidence type is specified, and as \enquote{not investigated} otherwise. This preserves the intended three-level outcome structure.} When investigated, evidence is recorded as production, perception, or metalinguistic behaviors. In modeling, \enquote{tested} corresponds to \enquote{investigated}, and \enquote{found} is defined conditional on testing. Note that \enquote{found} means \enquote{reported as found}; the category \enquote{investigated but not found} is heterogeneous, encompassing genuinely absent effects, underpowered tests, and mismatched social-meaning targets. Because \enquote{both} is a coding convention rather than a theoretical category, a sensitivity check counts such variables in both realization and order categories; this doesn't change the qualitative selection pattern.

\subsection{Why a selection model?}
Sociolinguistic theory has long debated whether grammatical variables carry social meaning. To answer this, we typically look to the published record. But that record answers a different question: among variables that researchers chose to test, which showed effects? Variables that nobody tested are invisible to any analysis that conditions on the tested subset.

Consider the dative alternation: \emph{I gave a book to her} vs. \emph{I gave her a book}. Can listeners hear a social difference between these forms? Do speakers use them to signal identity? To answer this, researchers must first choose to test the variable. If order variables are rarely tested~-- perhaps because they're harder to elicit or less salient to researchers~-- then even a null result in the published record tells us little about their social-meaning potential. This creates a form of survivor bias. We're trying to learn about the fleet by inspecting only the planes that made it home. A \term{selection model} makes this explicit by estimating two things at once: the probability that a variable gets tested, and the probability that a tested variable shows an effect. The goal isn't to infer what would happen if \emph{all} variables were tested, but to decompose the published record into (i) selection into testing and (ii) outcomes conditional on selection.

\subsection{Modeling strategy}
The model has two stages. The \term{selection stage} asks: given a variable--variety pairing in the corpus, was social significance tested? The \term{outcome stage} asks: given that testing occurred, was social significance found? This structure separates research-practice effects (who gets tested) from capacity effects (who shows effects once tested).

Readers familiar with mixed-effects logistic regression~-- the workhorse of variable-rule analysis in tools like Rbrul or Goldvarb~-- will recognize the outcome stage. It's the same model: a binary outcome (found or not) predicted by fixed effects (journal, variation type, year) and random effects (paper, author, language). What's new is pairing it with a selection stage that asks why some variables were tested at all.

Predictors in both stages are journal, variation type, and year. Year is $z$-scored (centered and scaled), so year odds ratios represent the change associated with a one-standard-deviation increase in publication year~-- roughly a decade in this corpus. Random intercepts for paper, first author, and language account for clustering, and correlations across stages allow the same paper or author to influence both testing and findings.

The model is fit in Stan, a probabilistic programming language for Bayesian inference. Priors are regularizing: Normal$(0, 1.0)$ for intercepts, Normal$(0, 0.5)$ for slopes, Exponential$(3)$ for random-effect standard deviations, and LKJ$(2)$ for correlation matrices. A sensitivity run centers intercept priors on observed marginal rates. Sampling uses four chains, \liningnums{4000} iterations (\liningnums{2000} warmup), and a brms two-stage baseline provides comparison.

\subsection{Reading the output}
Results are presented as odds ratios (ORs) and predicted probabilities. An OR of 2 means twice the odds; an OR of 0.5 means half the odds. Odds ratios are notoriously hard to intuit~-- and often make readers wish for predicted probabilities~-- so we provide those too (Figure~\ref{fig:probabilities}). A 95\% CrI is an interval containing 95\% of the posterior probability mass. Rather than focusing on whether an interval \enquote{excludes 1}, we emphasize the predictive implications: how much does the probability of testing or finding an effect change when we shift from realization to order variables? All fixed effects are treatment-coded with JSlx and \term{both} as reference levels, and year centered at the corpus mean.

Posterior predictive checks (PPCs) ask: \enquote{Can the model reproduce data like ours?} If simulated data from the fitted model cluster around the observed data, the model is capturing the key structure. If the model can't convincingly fake our dataset, it shouldn't be trusted to explain it.

One feature of Bayesian models with random effects is \term{partial pooling}: estimates for groups with little data are pulled toward the overall mean. This protects against overfitting small samples. Wide intervals for sparse categories (like order variables in perception studies) should be read as \enquote{data can't resolve this}, not \enquote{effect is absent.}

This analysis reports credible intervals rather than $p$-values. Bayesian inference asks \enquote{how probable are different parameter values given the data?} rather than \enquote{how surprising would the data be under a null hypothesis?} There's no yes/no significance threshold. Instead, intervals convey uncertainty directly: narrow intervals mean the data are informative; wide intervals mean they're not. This framing keeps uncertainty explicit and avoids a hard significance threshold.

\section{Results}

\subsection{Model fit and predictive checks}
Before interpreting the results, it's worth asking: does the model fit the data well? Posterior predictive checks (PPCs) answer this by simulating new datasets from the fitted model and asking whether they resemble the observed data. Here, the replicated tested and found rates cluster around the observed rates overall and within journal and variation-type strata (Figures~\ref{fig:ppc-overall} and~\ref{fig:ppc-stratified}). The observed rates fall well inside the predictive distributions, suggesting that the model captures the main selection structure without apparent overfitting. Population-level predicted probabilities make the selection patterns easier to read (Figure~\ref{fig:probabilities}).

\begin{figure}[t]
  \centering
  \includegraphics[width=\textwidth]{ppc-overall.png}
  \caption{Posterior predictive checks for overall tested and found rates. Each histogram shows rates from 500 simulated datasets; vertical lines mark the observed rates. If the model fits well, the observed rate should fall near the center of the distribution, not in the tails.}
  \label{fig:ppc-overall}
\end{figure}

\begin{figure}[t]
  \centering
  \includegraphics[width=\textwidth]{ppc-stratified.png}
  \caption{Stratified posterior predictive checks by journal and variation type. The same logic applies within subgroups: if observed rates (vertical lines) fall inside the simulated distributions, the model captures the subgroup structure.}
  \label{fig:ppc-stratified}
\end{figure}

\begin{figure}[t]
  \centering
  \includegraphics[width=\textwidth]{probabilities.png}
  \caption{Population-level predicted probabilities of testing (top) and finding significance (bottom) by journal and variation type. Points show posterior medians; error bars show 95\% credible intervals. Non-overlapping intervals suggest reliable differences; overlapping intervals suggest uncertainty about whether groups differ.}
  \label{fig:probabilities}
\end{figure}

\subsection{Selection (tested)}
Odds ratios (ORs) and credible intervals (CrIs) summarize the selection stage. Values above 1 mean higher odds of being tested; values below 1 mean lower odds.

Testing varies by journal, variation type, and year:
\begin{itemize}
  \item \textbf{Journal}: LVC is less likely to test variables than JSlx (OR $\approx 0.37$, 95\% CrI 0.17--0.83). In plain terms, variables in LVC have about one-third the odds of being tested for social significance compared to JSlx.
  \item \textbf{Variation type}: Realization variables are more likely to be tested than order variables (OR $\approx 2.33$, 95\% CrI 1.12--4.89). Realization variables have more than twice the odds of being tested.
  \item \textbf{Year}: Testing increases over time (OR $\approx 1.65$, 95\% CrI 1.04--2.67). For each decade, testing odds increase by about 65\%.
\end{itemize}
The order--both contrast remains uncertain, with a wide interval spanning no effect.

\subsection{Outcome (found \texorpdfstring{$\mid$}{|} tested)}
Conditional on testing, success rates are high and comparable across types. The probability of finding social significance is high (approximately 88--94\% across subgroups) and shows wide overlap across journals and variation types. Outcome effects have intervals that span no effect, consistent with selection rather than capacity as the bottleneck. Wide credible intervals in sparse categories~-- order variables, perception and metalinguistic domains~-- reflect data limitations, not absence of effect. The model's partial pooling shrinks extreme estimates toward the population mean, so intervals spanning no effect should be read as \enquote{data can't resolve this}, not \enquote{effect is absent.}

\subsection{Robustness}
A good analysis shouldn't depend on arbitrary modeling choices. Several robustness checks confirm that the main findings are stable:

The brms two-stage baseline yields the same qualitative pattern: selection effects are stable, outcome effects are weak. The centered-intercept sensitivity run~-- which uses priors centered on observed rates rather than agnostic priors~-- reproduces the same conclusions.

An extended model adds a journal $\times$ variation-type interaction in the selection stage. If institutional framing requirements drive selection, JSlx might show a larger gap between realization and order than LVC. The interaction coefficients represent the multiplicative change in the LVC effect for each variation type relative to the reference (both). These coefficients have wide intervals spanning no effect (order:LVC OR $\approx 0.54$, 95\% CrI 0.23--1.29; realization:LVC OR $\approx 0.66$, 95\% CrI 0.30--1.47), so the data don't clearly support differential selection by journal.

A piecewise-linear year effect (two interior knots) tests whether selection trends are non-linear. The spline coefficients point in the same direction (ORs 1.19, 1.72, 1.32; all intervals spanning 1), consistent with the primary model's linear year assumption rather than distinct period effects.

\textbf{Summary}: Selection effects are robust across modeling choices. The outcome effects remain weak and uncertain regardless of how the model is specified, strengthening the conclusion that selection~-- not capacity~-- drives the published pattern.

\section{Discussion}

\subsection{Selection vs. outcome}
The main asymmetry is in selection. Journals, variable type, and time shape which variables are tested, but tested variables show high and overlapping probabilities of social significance. This supports a selection-driven account of the literature: the bottleneck is which variables are studied, not whether tested variables yield positive findings once investigated. This conclusion concerns publication and research-design practices rather than the underlying capacity of grammatical variables to carry social meaning.

\subsection{Implications for the Grammatical Invisibility Principle}
The Grammatical Invisibility Principle (GIP) holds that grammatical variables are less accessible to social evaluation than phonological variables. \textcite{mackenzie2025spelling} conclude that for realization variables, robust evidence of social significance exists; for order variables, absence of evidence doesn't equal evidence of absence. The present analysis sharpens this: \emph{conditional on testing}, both realization and order variables succeed at comparable rates (88--94\%). Note that this high success rate likely reflects a strong filter: once researchers choose to test a variable, they overwhelmingly report finding effects. A success rate near 90\% is either a triumph of sociolinguistic detection or a clue about which results survive the trip to print; the model cannot separate these. This suggests that the tested variables may be positively selected for plausibility. In other words, we may be looking at the order-variable honour roll, not the whole graduating class. Thus, the finding undercuts the strong version of the GIP~-- that order variables \emph{can't} carry social meaning~-- while remaining agnostic about whether order variables are \emph{harder to perceive} or \emph{less frequently recruited}. On a causal-network view, the relevant question is not whether order variables \emph{can} carry social meaning, but which causal pathways (e.g., perceptual salience, frequency, register mediation) make them projectible targets for social evaluation. The current literature is sparse precisely where those pathways would need to be mapped.

\subsection{Relation to prior descriptive work}
The descriptive goal of \textcite{mackenzie2025spelling} is to catalog grammatical variables and assess the state of evidence for social significance in each domain. The present analysis retains their coding scheme and tallies without modification. The points of agreement are substantial: realization variables dominate the literature; order variables are understudied; perception and metalinguistic domains remain sparse.

The focused revision is methodological. MacKenzie and Robinson devote considerable discussion (their Section 5) to selection into testing, including the file drawer problem and the fact that LVC doesn't require a social-significance framing. They explicitly warn against treating the scarcity of order-variable evidence as evidence that such effects don't exist, emphasizing that the relevant studies have largely not been done. What the present analysis adds is an explicit model for this selection mechanism. Rather than treating untested variables as missing data to be ignored, the joint model estimates the probability of testing and conditions the outcome analysis on testing. This formalizes what MacKenzie and Robinson discuss narratively.

The model's selection estimates converge with their descriptive findings. They report that among production studies, LVC finds social significance at 92\% (143/156) compared to JSlx at 83\% (87/105). This difference ($\chi^2=4.65$, $p=.031$) supports their argument that LVC's lack of a social-significance framing requirement doesn't inflate publication bias~-- researchers who look for effects overwhelmingly find them.

The novelty is thus formalization rather than conceptual revision. The selection--outcome decomposition provides a principled statistical framework for claims that MacKenzie and Robinson already make informally. The difference in emphasis is slight: the high baseline success rate (~88--94\%) is the story, and the between-type comparison is null in part because of ceiling effects and limited order data.

\subsection{Limitations and future work}
Order variables are rare and heavily concentrated in LVC~-- the journal that is \emph{less} likely to test for social significance. This constrains journal comparisons. With only 31 tested order variables, power to detect plausible outcome-stage differences is limited. Furthermore, the binary \enquote{found} outcome collapses heterogeneity in evidence strength (p-values, authors' qualitative judgments), and high success rates likely reflect publication bias. Finally, because the database indexes published studies, the model can't address file-drawer effects where testing occurs but null results aren't published. The model describes the published record, which acts as a filter on the underlying linguistic reality.

MacKenzie and Robinson also call for collaboration with formal syntacticians to map realization variables to Spell-Out and order variables to Narrow Syntax. This architectural distinction might predict sociolinguistic differences beyond what the current selection model captures. Future work should expand the corpus beyond two journals, target perception and metalinguistic studies for order variables, and explore whether formal-syntactic classifications improve predictive power.

\section*{Acknowledgements}

ChatGPT 5.2 and Claude Opus 4.5 assisted with analysis and drafting; the author is responsible for all claims and any errors.

\newpage
\appendix
\section{Model specification}

The joint selection--outcome model has two stages. For observation $i$:

\textbf{Selection stage:}
\begin{equation}
\text{tested}_i \sim \text{Bernoulli}(\text{logit}^{-1}(\eta^s_i))
\end{equation}
\begin{equation}
\eta^s_i = \alpha^s + \mathbf{x}_i \boldsymbol{\beta}^s + u^s_{\text{paper}[i]} + u^s_{\text{author}[i]} + u^s_{\text{language}[i]}
\end{equation}

\textbf{Outcome stage} (conditional on $\text{tested}_i = 1$):
\begin{equation}
\text{found}_i \mid \text{tested}_i = 1 \sim \text{Bernoulli}(\text{logit}^{-1}(\eta^o_i))
\end{equation}
\begin{equation}
\eta^o_i = \alpha^o + \mathbf{x}_i \boldsymbol{\beta}^o + u^o_{\text{paper}[i]} + u^o_{\text{author}[i]} + u^o_{\text{language}[i]}
\end{equation}

where $\mathbf{x}_i$ is the row of the fixed-effects design matrix containing journal, variation type, and year ($z$-scored).

\textbf{Random effects:} For each grouping factor $g \in \{\text{paper}, \text{author}, \text{language}\}$:
\begin{equation}
\begin{pmatrix} u^s_{g[i]} \\ u^o_{g[i]} \end{pmatrix} \sim \text{MVN}\left(\mathbf{0}, \boldsymbol{\Sigma}_g\right), \quad \boldsymbol{\Sigma}_g = \begin{pmatrix} (\sigma^s_g)^2 & \rho_g \sigma^s_g \sigma^o_g \\ \rho_g \sigma^s_g \sigma^o_g & (\sigma^o_g)^2 \end{pmatrix}
\end{equation}

\textbf{Priors:}
\begin{align}
\alpha^s, \alpha^o &\sim \text{Normal}(0, 1.0) \\
\beta^s_k, \beta^o_k &\sim \text{Normal}(0, 0.5) \\
\sigma^s_g, \sigma^o_g &\sim \text{Exponential}(3) \\
\mathbf{L}_g &\sim \text{LKJ-Cholesky}(2)
\end{align}

where $\mathbf{L}_g$ is the Cholesky factor of the correlation matrix for grouping factor $g$.

\newpage
\printbibliography

\end{document}
