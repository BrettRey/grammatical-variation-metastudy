---
title: "Analysis Plan: Grammatical Variation Meta-Study Reanalysis"
author: "Brett Reynolds"
date: "2026-01-10"
format:
  html:
    toc: true
    code-fold: true
---

## Overview

This document outlines a Gelman-style Bayesian reanalysis of MacKenzie & Robinson (2025), which catalogued grammatical variables from *Language Variation and Change* and *Journal of Sociolinguistics* through 2023.

## Why Reanalyze?

The original analysis uses simple proportions and contingency table tests (Fisher's exact, chi-squared). This approach:

1. **Treats observations as independent** - ignoring clustering by paper, author, language
2. **Binarizes outcomes** - collapsing "not analyzed / null / found" into binary comparisons
3. **Conflates selection with outcomes** - doesn't model *why* some variables get tested

A multilevel Bayesian approach addresses all three.

## Data Structure

```
426 variable-variety combinations
├── Clustered by paper (~N papers)
├── Clustered by author(s) (~N unique author combinations)
├── Clustered by language (~N languages)
├── Clustered by journal (2 journals)
└── Varying by year (1990s-2023)
```

**First task:** EDA to determine clustering counts.

## Model Architecture

### Model 1: Joint Selection–Outcome Model (Primary)

We model selection into testing and the outcome of testing jointly to capture selection bias directly.

**Selection (tested vs not analyzed):**
$$
\text{logit}(P(\text{tested}_i)) = \alpha_s + \beta_s X_i + u^{(s)}_{\text{paper}[i]} + u^{(s)}_{\text{author}[i]} + u^{(s)}_{\text{language}[i]} + u^{(s)}_{\text{journal}[i]}
$$

**Outcome (found vs null | tested):**
$$
\text{logit}(P(\text{found}_i \mid \text{tested}_i)) = \alpha_o + \beta_o X_i + u^{(o)}_{\text{paper}[i]} + u^{(o)}_{\text{author}[i]} + u^{(o)}_{\text{language}[i]} + u^{(o)}_{\text{journal}[i]}
$$

Correlation between selection and outcome is handled via **shared or correlated varying effects** (and, if needed, a custom Stan bivariate selection model).

**Predictors (both stages):**
- Variation type (realization vs order vs both)
- Journal (2 levels)
- Year (linear)

**Random effects (both stages):**
- Paper
- Author
- Language

### Model 2: Domain-Specific Joint Models

Fit the same joint selection–outcome structure separately for each domain:
- Production (`prod_tested`, `prod_found`)
- Perception (`perc_tested`, `perc_found`)
- Metalinguistic behavior (`meta_tested`, `meta_found`)

**Key question:** Is the evidence base structurally skewed toward production?

### Model 3: Baseline Two-Stage Models (Sensitivity)

For interpretability checks, fit the two-stage models separately (selection then outcome) without explicit correlation. Use these only for comparison against the joint model.

## Implementation

### Software
- `brms` for multilevel joint models where possible
- `cmdstanr` if custom Stan is needed for the joint selection specification
- `tidybayes` for posterior visualization

### Workflow
1. Data prep and EDA
2. Fit joint selection–outcome model (overall)
3. Fit domain-specific joint models
4. Fit baseline two-stage models for sensitivity
5. Compare to original findings
6. Sensitivity analysis

## Questions to Answer

1. **How much variation in "found social significance" is attributable to:**
   - Variable type (realization vs order)?
   - Journal (LVC vs JSlx)?
   - Language?
   - Individual paper/author effects?

2. **Is the literature biased toward production studies?**
   - What's the selection rate for production vs perception vs metalinguistic?
   - Conditional on testing, do success rates differ?

3. **Does the original conclusion hold under multilevel analysis?**
   - Original: realization variables more often show social significance than order variables
   - Does this survive controlling for clustering?

## Predictors & Recodes (Explicit)

- **Variation type recode:** if `variation.type` contains `", order"` → `both`; else if it contains `form` or `omission` → `realization`; else `order`.
- **Year:** use a linear term (centered and scaled) for the main models.

## Priors

Use weakly informative priors (regularizing, not subjective):
- Fixed effects (standardized predictors): $\beta \sim \text{Normal}(0, 0.5)$ on log-odds scale
- Intercepts: $\alpha \sim \text{Normal}(0, 1.0)$
- Varying intercept SDs: $\sigma_u \sim \text{Exponential}(3)$
- Correlations among varying effects: LKJ(2) prior (when modeled)

## Diagnostics

- Posterior predictive checks
- LOO-CV for model comparison, with Pareto-$k$ diagnostics (trigger `reloo` if needed)
- Trace plots, $\hat{R}$, ESS

## Gelman-Style Workflow Checklist

1. **Prior predictive check** before any fitting (sanity on baseline rates and variability).
2. **Fit the simplest joint model** that matches the research question.
3. **Posterior predictive check** to validate fit to observed distributions.
4. **Incremental complexity** only when simpler models misfit key patterns.
5. **Model comparison with diagnostics** (LOO/ELPD + Pareto-$k$).
6. **Sensitivity analysis** for coding choices and priors.

## Sensitivity Analyses

- **NA coding:** treat NA as `null` vs `not analyzed` vs missing (if data owner confirms).
- **Year trend:** linear vs smooth.
- **Selection structure:** joint vs two-stage.
- **Prior scale:** tighter vs wider (e.g., `Normal(0, 0.7)` vs `Normal(0, 1.5)` for slopes).
- **Intercept centering:** agnostic intercepts vs data-centered intercepts (sensitivity only).

## Timeline

Not estimated (per portfolio conventions).

## References

Gelman, A., Hill, J., & Vehtari, A. (2020). *Regression and Other Stories*. Cambridge University Press.

MacKenzie, L., & Robinson, M. (2025). Spelling out grammatical variation. In D. Duncan & M. Robinson (Eds.), *English Sociosyntax: Theory, Evidence, Approaches* (pp. 59-95). De Gruyter Mouton.

McElreath, R. (2020). *Statistical Rethinking* (2nd ed.). CRC Press.
